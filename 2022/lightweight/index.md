# 轻量化网络内容小结


## ADDerNet  
Q:在硬件设备上加法的复杂度远小于乘法, 为更好地减少计算量提出AdderNet.
- 对AdderNet中的Winograd算法 **（广泛用于加速卷积和节省计算量。由于乘法中的分配律对L1范式无效，因此原始的W算法无法直接在AdderNet上使用）** 进行优化，并 **（在Winograd算法中）** 使用相加操作替换逐像素相乘。
- 分析AdderNet中Winograd的特征图不均衡的原因，提出了最大程度地增强新输出特征的特征表示能力的最佳变换矩阵。此外，提出l2-l1训练策略来适应 Winograd AdderNet 范式并避免网络性能下降。
- 实验证明，AdderNet在FPGA中的计算量比baseline减少2.1倍，并保持与baseline相当的性能。

---
## GhostNet  
Q:由于嵌入式设备有限的内存和计算资源，导致CNN难以在嵌入式设备上得到应用。 
- Ghost Moudle **（一个即插即用的模块）** ：将常规卷积层分为两部分，第一部分是传统卷积层，但是数量受到严格控制；对第一部分得到的特征图再使用一系列简单地线性操作来生成更多的特征图。在不改变特征图尺寸的条件下，GM相较于普通的卷积减少大量参数量和计算复杂度。
- GhonstNet：基于GM建立了有效的GNet。其中，首次使用GM替换网络中的常规卷积层来证明 Ghost 模块的有效性，然后在几个数据集上验证GN的性能。  
- 实验证明GM相较于常规卷积，在减少计算量的情况下还保持了相当的性能。

---
## GPUNet  
Q：之前的网络在优化网络结构的同时也引入更多的参数，并低效的生成有效的特征图。  
- GP-moudle 
- GPU-Net：一个轻量的UNet在有效减少参数量和FLOPs的情况下保持相当甚至更好地性能。也是第一篇探索将GM及其变种应用与UNet的论文。

---
## Lite-HRNet  
Q：  
- 在HRNet中使用shuffleNet中的Shuffle block。性能超过ShuffleNet、MobileNet。
- 引入轻量的条件通道加权单元 **(权重从HRNet的并行分支中所有的通道和分辨率中学习得到)** 来替换shuffle block中的点卷积，来减少计算量 **(通道加权的复杂度与通道数成线性关系，低于逐点卷积的二次时间复杂度。)** 。它使用权重作为跨通道和分辨率交换信息的桥梁，补偿逐点（1×1）卷积所起的作用。
---

## RegSeg  
Q：现有的成功的网络极大依赖大计算量，如果没有足够大的感受野就无法作出最佳的选择。  
- 受ResNeXt启发，设计了一个块结构。该结构并行使用两个有不同膨胀率的卷积，在增大网络感受野的同时保留局部信息。通过堆叠使用这个模块，可以在不使用额外计算量的条件下控制网络感受野。
- 提出一个轻量解码器，相较于常规解码器，该解码器能更好地恢复局部信息。  

---
## RepMLP  
Q: 全连接层在长效依赖和位置信息建模方面表现优越，但是在捕获局部信息方面较弱。  
- 提出利用 FC 的全局信息捕获能力和位置感知，并为其配备局部先验 **(在RepMLP中的FC层中建立卷积层进行提取)** 以进行图像识别。
- 提出了一种简单、与平台无关且可微分的算法，将并行 conv 和 BN 合并到局部先验的 FC 中，并且不需要消耗推理时间。
- 提出RepMLP，在许多视觉任务上都展示了其性能。  

---  
## Swin-Unet  
Q：尽管CNN有出色的表现，但是由于卷积操作的局部性，它不能很好的学习到全局信息和长效语义信息。  
- 基于Swin Transformer block，设计了对称的带跳跃连接的编解码结构。在编码器中实现了从局部到全局的自注意力机制，在解码器全局特征被上采样到输入尺寸大小来得到最终的分割结果。  
- 开发了一个patch扩展层，在不使用卷积或插值操作的情况下实现上采样和特征维数增加。 
- 建立纯Transformer的U形带跳跃连接结构，命名为SwinUnet。

---
## UNeXt  
Q:现有的U型网络（TransUnet）不能有效地用于即时应用中的快速图像分割，因为它们参数繁重、计算复杂且使用缓慢。
- 提出UNeXt，第一个基于卷积MLP的图像分割网络。  
- 提出了一种轴向位移的tokenized MLP模块，有效地学习潜在空间的良好表示。
- 成功提高医学图像分割任务性能的同时减少参数量、提高推理速度并降低计算复杂度。
































